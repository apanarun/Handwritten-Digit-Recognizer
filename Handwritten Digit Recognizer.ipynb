{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Importing the Necessary Modules"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import keras\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras import backend as K\n\n# the data, split between train and test sets\n(x_train,y_train),(x_test,y_test) = mnist.load_data()\n\nprint(x_train.shape,y_train.shape)","execution_count":2,"outputs":[{"output_type":"stream","text":"Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n11493376/11490434 [==============================] - 0s 0us/step\n","name":"stdout"},{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"},{"output_type":"stream","text":"(60000, 28, 28) (60000,)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# 2. Preprocess the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = x_train.reshape(x_train.shape[0],28,28,1)\nx_test = x_test.reshape(x_test.shape[0],28,28,1)\ninput_shape = (28, 28, 1)\n\n# convert class vectors to binary class matrices\nnum_classes = 10\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\n\nprint(x_train.shape)\nx_test.shape\n","execution_count":3,"outputs":[{"output_type":"stream","text":"(60000, 28, 28, 1)\n","name":"stdout"},{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"(10000, 28, 28, 1)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# 3. Importing Gpu's and Tpu's"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Detect hardware\nimport tensorflow as tf\ntry:\n  tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\nexcept ValueError:\n  tpu = None\n  gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n    \n# Select appropriate distribution strategy\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    # instantiate a distribution strategy\n    strategy = tf.distribute.experimental.TPUStrategy(tpu) \n    print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])  \nelif len(gpus) > 1:\n  strategy = tf.distribute.MirroredStrategy([gpu.name for gpu in gpus])\n  print('Running on multiple GPUs ', [gpu.name for gpu in gpus])\nelif len(gpus) == 1:    \n  strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n  print('Running on single GPU ', gpus[0].name)\nelse:\n  strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n  print('Running on CPU')\nprint(\"Number of accelerators: \", strategy.num_replicas_in_sync)","execution_count":4,"outputs":[{"output_type":"stream","text":"Running on TPU  ['10.0.0.2:8470']\nNumber of accelerators:  8\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# 4. Create the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parameters\nBATCH_SIZE = 64 * strategy.num_replicas_in_sync \nLEARNING_RATE = 0.01\nLEARNING_RATE_EXP_DECAY = 0.6 if strategy.num_replicas_in_sync == 1 else 0.7","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make Model\ndef make_model():\n    model = tf.keras.Sequential(\n      [\n        tf.keras.layers.Reshape(input_shape=(28*28,), target_shape=(28, 28, 1), name=\"image\"),\n\n        tf.keras.layers.Conv2D(filters=12, kernel_size=3, padding='same', use_bias=False), \n        tf.keras.layers.BatchNormalization(scale=False, center=True), \n        tf.keras.layers.Activation('relu'), \n\n        tf.keras.layers.Conv2D(filters=24, kernel_size=6, padding='same', use_bias=False, strides=2),\n        tf.keras.layers.BatchNormalization(scale=False, center=True),\n        tf.keras.layers.Activation('relu'),\n\n        tf.keras.layers.Conv2D(filters=32, kernel_size=6, padding='same', use_bias=False, strides=2),\n        tf.keras.layers.BatchNormalization(scale=False, center=True),\n        tf.keras.layers.Activation('relu'),\n\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(200, use_bias=False),\n        tf.keras.layers.BatchNormalization(scale=False, center=True),\n        tf.keras.layers.Activation('relu'),\n        tf.keras.layers.Dropout(0.4), # Dropout on dense layer only\n\n        tf.keras.layers.Dense(10, activation='softmax')\n      ])\n\n    model.compile(optimizer='adam', # learning rate will be set by LearningRateScheduler\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n    return model","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating the Model\nwith strategy.scope():\n    model = make_model()\n\n# print model layers\nmodel.summary()\n\n# set up learning rate decay\nlr_decay = tf.keras.callbacks.LearningRateScheduler(\n    lambda epoch: LEARNING_RATE * LEARNING_RATE_EXP_DECAY**epoch,\n    verbose=True)","execution_count":7,"outputs":[{"output_type":"stream","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nimage (Reshape)              (None, 28, 28, 1)         0         \n_________________________________________________________________\nconv2d (Conv2D)              (None, 28, 28, 12)        108       \n_________________________________________________________________\nbatch_normalization (BatchNo (None, 28, 28, 12)        36        \n_________________________________________________________________\nactivation (Activation)      (None, 28, 28, 12)        0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 14, 14, 24)        10368     \n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 14, 14, 24)        72        \n_________________________________________________________________\nactivation_1 (Activation)    (None, 14, 14, 24)        0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 7, 7, 32)          27648     \n_________________________________________________________________\nbatch_normalization_2 (Batch (None, 7, 7, 32)          96        \n_________________________________________________________________\nactivation_2 (Activation)    (None, 7, 7, 32)          0         \n_________________________________________________________________\nflatten (Flatten)            (None, 1568)              0         \n_________________________________________________________________\ndense (Dense)                (None, 200)               313600    \n_________________________________________________________________\nbatch_normalization_3 (Batch (None, 200)               600       \n_________________________________________________________________\nactivation_3 (Activation)    (None, 200)               0         \n_________________________________________________________________\ndropout (Dropout)            (None, 200)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 10)                2010      \n=================================================================\nTotal params: 354,538\nTrainable params: 354,002\nNon-trainable params: 536\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# 5. Train the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 10\nsteps_per_epoch = 60000//BATCH_SIZE  # 60,000 items in this dataset\nprint(\"Steps per epoch: \", steps_per_epoch)\n  \n# Little wrinkle: in the present version of Tensorfow (1.14), switching a TPU\n# between training and evaluation is slow (approx. 10 sec). For small models,\n# it is recommeneded to run a single eval at the end.\n\nhistory = model.fit(x_train,y_train,\n                    steps_per_epoch=steps_per_epoch, epochs=EPOCHS,\n                    callbacks=[lr_decay])\n\nfinal_stats = model.evaluate(x_test,y_test, steps=1)\nprint(\"Validation accuracy: \", final_stats[1])\n\nmodel.save('Handwritten-digit-mnist.h5')","execution_count":15,"outputs":[{"output_type":"stream","text":"Steps per epoch:  117\n\nEpoch 00001: LearningRateScheduler reducing learning rate to 0.01.\nEpoch 1/50\n116/117 [============================>.] - ETA: 0s - accuracy: 0.9911 - loss: 0.0313\nEpoch 00001: val_accuracy improved from -inf to 0.97900, saving model to best_model.h5\n117/117 [==============================] - 5s 44ms/step - accuracy: 0.9911 - loss: 0.0315 - val_accuracy: 0.9790 - val_loss: 0.1792 - lr: 0.0100\n\nEpoch 00002: LearningRateScheduler reducing learning rate to 0.006999999999999999.\nEpoch 2/50\n114/117 [============================>.] - ETA: 0s - accuracy: 0.9930 - loss: 0.0223\nEpoch 00002: val_accuracy improved from 0.97900 to 0.97990, saving model to best_model.h5\n117/117 [==============================] - 5s 46ms/step - accuracy: 0.9931 - loss: 0.0220 - val_accuracy: 0.9799 - val_loss: 0.0861 - lr: 0.0070\n\nEpoch 00003: LearningRateScheduler reducing learning rate to 0.0049.\nEpoch 3/50\n116/117 [============================>.] - ETA: 0s - accuracy: 0.9971 - loss: 0.0087\nEpoch 00003: val_accuracy improved from 0.97990 to 0.99360, saving model to best_model.h5\n117/117 [==============================] - 5s 46ms/step - accuracy: 0.9970 - loss: 0.0088 - val_accuracy: 0.9936 - val_loss: 0.0214 - lr: 0.0049\n\nEpoch 00004: LearningRateScheduler reducing learning rate to 0.003429999999999999.\nEpoch 4/50\n114/117 [============================>.] - ETA: 0s - accuracy: 0.9983 - loss: 0.0049\nEpoch 00004: val_accuracy did not improve from 0.99360\n117/117 [==============================] - 5s 44ms/step - accuracy: 0.9983 - loss: 0.0049 - val_accuracy: 0.9932 - val_loss: 0.0220 - lr: 0.0034\n\nEpoch 00005: LearningRateScheduler reducing learning rate to 0.0024009999999999995.\nEpoch 5/50\n116/117 [============================>.] - ETA: 0s - accuracy: 0.9988 - loss: 0.0043\nEpoch 00005: val_accuracy did not improve from 0.99360\n117/117 [==============================] - 5s 45ms/step - accuracy: 0.9988 - loss: 0.0042 - val_accuracy: 0.9935 - val_loss: 0.0245 - lr: 0.0024\n\nEpoch 00006: LearningRateScheduler reducing learning rate to 0.0016806999999999994.\nEpoch 6/50\n117/117 [==============================] - ETA: 0s - accuracy: 0.9991 - loss: 0.0031\nEpoch 00006: val_accuracy improved from 0.99360 to 0.99410, saving model to best_model.h5\n117/117 [==============================] - 5s 42ms/step - accuracy: 0.9991 - loss: 0.0031 - val_accuracy: 0.9941 - val_loss: 0.0209 - lr: 0.0017\n\nEpoch 00007: LearningRateScheduler reducing learning rate to 0.0011764899999999997.\nEpoch 7/50\n116/117 [============================>.] - ETA: 0s - accuracy: 0.9992 - loss: 0.0027\nEpoch 00007: val_accuracy improved from 0.99410 to 0.99440, saving model to best_model.h5\n117/117 [==============================] - 5s 43ms/step - accuracy: 0.9992 - loss: 0.0027 - val_accuracy: 0.9944 - val_loss: 0.0208 - lr: 0.0012\n\nEpoch 00008: LearningRateScheduler reducing learning rate to 0.0008235429999999996.\nEpoch 8/50\n116/117 [============================>.] - ETA: 0s - accuracy: 0.9993 - loss: 0.0022\nEpoch 00008: val_accuracy did not improve from 0.99440\n117/117 [==============================] - 5s 42ms/step - accuracy: 0.9993 - loss: 0.0022 - val_accuracy: 0.9943 - val_loss: 0.0214 - lr: 8.2354e-04\n\nEpoch 00009: LearningRateScheduler reducing learning rate to 0.0005764800999999997.\nEpoch 9/50\n116/117 [============================>.] - ETA: 0s - accuracy: 0.9994 - loss: 0.0022\nEpoch 00009: val_accuracy improved from 0.99440 to 0.99450, saving model to best_model.h5\n117/117 [==============================] - 5s 44ms/step - accuracy: 0.9994 - loss: 0.0023 - val_accuracy: 0.9945 - val_loss: 0.0213 - lr: 5.7648e-04\n\nEpoch 00010: LearningRateScheduler reducing learning rate to 0.0004035360699999998.\nEpoch 10/50\n115/117 [============================>.] - ETA: 0s - accuracy: 0.9994 - loss: 0.0022\nEpoch 00010: val_accuracy did not improve from 0.99450\n117/117 [==============================] - 5s 43ms/step - accuracy: 0.9994 - loss: 0.0022 - val_accuracy: 0.9940 - val_loss: 0.0218 - lr: 4.0354e-04\n\nEpoch 00011: LearningRateScheduler reducing learning rate to 0.00028247524899999984.\nEpoch 11/50\n116/117 [============================>.] - ETA: 0s - accuracy: 0.9995 - loss: 0.0018\nEpoch 00011: val_accuracy did not improve from 0.99450\n117/117 [==============================] - 5s 42ms/step - accuracy: 0.9995 - loss: 0.0018 - val_accuracy: 0.9942 - val_loss: 0.0216 - lr: 2.8248e-04\n\nEpoch 00012: LearningRateScheduler reducing learning rate to 0.0001977326742999999.\nEpoch 12/50\n116/117 [============================>.] - ETA: 0s - accuracy: 0.9995 - loss: 0.0019\nEpoch 00012: val_accuracy did not improve from 0.99450\n117/117 [==============================] - 5s 42ms/step - accuracy: 0.9995 - loss: 0.0019 - val_accuracy: 0.9941 - val_loss: 0.0215 - lr: 1.9773e-04\n\nEpoch 00013: LearningRateScheduler reducing learning rate to 0.0001384128720099999.\nEpoch 13/50\n116/117 [============================>.] - ETA: 0s - accuracy: 0.9994 - loss: 0.0019\nEpoch 00013: val_accuracy did not improve from 0.99450\n117/117 [==============================] - 5s 42ms/step - accuracy: 0.9994 - loss: 0.0019 - val_accuracy: 0.9943 - val_loss: 0.0215 - lr: 1.3841e-04\n\nEpoch 00014: LearningRateScheduler reducing learning rate to 9.688901040699991e-05.\nEpoch 14/50\n116/117 [============================>.] - ETA: 0s - accuracy: 0.9994 - loss: 0.0019\nEpoch 00014: val_accuracy did not improve from 0.99450\n117/117 [==============================] - 5s 43ms/step - accuracy: 0.9994 - loss: 0.0019 - val_accuracy: 0.9944 - val_loss: 0.0217 - lr: 9.6889e-05\n\nEpoch 00015: LearningRateScheduler reducing learning rate to 6.782230728489995e-05.\nEpoch 15/50\n116/117 [============================>.] - ETA: 0s - accuracy: 0.9995 - loss: 0.0018\nEpoch 00015: val_accuracy did not improve from 0.99450\n117/117 [==============================] - 5s 43ms/step - accuracy: 0.9995 - loss: 0.0018 - val_accuracy: 0.9945 - val_loss: 0.0217 - lr: 6.7822e-05\n\nEpoch 00016: LearningRateScheduler reducing learning rate to 4.7475615099429956e-05.\nEpoch 16/50\n116/117 [============================>.] - ETA: 0s - accuracy: 0.9996 - loss: 0.0016\nEpoch 00016: val_accuracy did not improve from 0.99450\n117/117 [==============================] - 5s 45ms/step - accuracy: 0.9996 - loss: 0.0016 - val_accuracy: 0.9944 - val_loss: 0.0217 - lr: 4.7476e-05\n\nEpoch 00017: LearningRateScheduler reducing learning rate to 3.323293056960097e-05.\nEpoch 17/50\n117/117 [==============================] - ETA: 0s - accuracy: 0.9996 - loss: 0.0019\nEpoch 00017: val_accuracy did not improve from 0.99450\n117/117 [==============================] - 5s 42ms/step - accuracy: 0.9996 - loss: 0.0019 - val_accuracy: 0.9943 - val_loss: 0.0216 - lr: 3.3233e-05\n\nEpoch 00018: LearningRateScheduler reducing learning rate to 2.3263051398720675e-05.\nEpoch 18/50\n116/117 [============================>.] - ETA: 0s - accuracy: 0.9995 - loss: 0.0019\nEpoch 00018: val_accuracy did not improve from 0.99450\n117/117 [==============================] - 5s 43ms/step - accuracy: 0.9995 - loss: 0.0019 - val_accuracy: 0.9943 - val_loss: 0.0216 - lr: 2.3263e-05\n\nEpoch 00019: LearningRateScheduler reducing learning rate to 1.628413597910447e-05.\nEpoch 19/50\n115/117 [============================>.] - ETA: 0s - accuracy: 0.9995 - loss: 0.0019\nEpoch 00019: val_accuracy did not improve from 0.99450\n117/117 [==============================] - 5s 43ms/step - accuracy: 0.9995 - loss: 0.0019 - val_accuracy: 0.9943 - val_loss: 0.0216 - lr: 1.6284e-05\n\nEpoch 00020: LearningRateScheduler reducing learning rate to 1.139889518537313e-05.\nEpoch 20/50\n116/117 [============================>.] - ETA: 0s - accuracy: 0.9996 - loss: 0.0018\nEpoch 00020: val_accuracy did not improve from 0.99450\n117/117 [==============================] - 5s 42ms/step - accuracy: 0.9996 - loss: 0.0018 - val_accuracy: 0.9942 - val_loss: 0.0216 - lr: 1.1399e-05\n\nEpoch 00021: LearningRateScheduler reducing learning rate to 7.97922662976119e-06.\nEpoch 21/50\n115/117 [============================>.] - ETA: 0s - accuracy: 0.9992 - loss: 0.0022\nEpoch 00021: val_accuracy did not improve from 0.99450\n117/117 [==============================] - 5s 42ms/step - accuracy: 0.9992 - loss: 0.0023 - val_accuracy: 0.9943 - val_loss: 0.0217 - lr: 7.9792e-06\n","name":"stdout"},{"output_type":"stream","text":"\nEpoch 00022: LearningRateScheduler reducing learning rate to 5.5854586408328325e-06.\nEpoch 22/50\n116/117 [============================>.] - ETA: 0s - accuracy: 0.9993 - loss: 0.0020\nEpoch 00022: val_accuracy did not improve from 0.99450\n117/117 [==============================] - 5s 41ms/step - accuracy: 0.9993 - loss: 0.0020 - val_accuracy: 0.9943 - val_loss: 0.0217 - lr: 5.5855e-06\n\nEpoch 00023: LearningRateScheduler reducing learning rate to 3.909821048582983e-06.\nEpoch 23/50\n116/117 [============================>.] - ETA: 0s - accuracy: 0.9994 - loss: 0.0022\nEpoch 00023: val_accuracy did not improve from 0.99450\n117/117 [==============================] - 5s 42ms/step - accuracy: 0.9994 - loss: 0.0022 - val_accuracy: 0.9943 - val_loss: 0.0215 - lr: 3.9098e-06\n\nEpoch 00024: LearningRateScheduler reducing learning rate to 2.736874734008088e-06.\nEpoch 24/50\n116/117 [============================>.] - ETA: 0s - accuracy: 0.9995 - loss: 0.0018\nEpoch 00024: val_accuracy did not improve from 0.99450\n117/117 [==============================] - 5s 42ms/step - accuracy: 0.9995 - loss: 0.0018 - val_accuracy: 0.9945 - val_loss: 0.0216 - lr: 2.7369e-06\n\nEpoch 00025: LearningRateScheduler reducing learning rate to 1.9158123138056613e-06.\nEpoch 25/50\n116/117 [============================>.] - ETA: 0s - accuracy: 0.9995 - loss: 0.0019\nEpoch 00025: val_accuracy did not improve from 0.99450\n117/117 [==============================] - 5s 43ms/step - accuracy: 0.9995 - loss: 0.0019 - val_accuracy: 0.9945 - val_loss: 0.0216 - lr: 1.9158e-06\n\nEpoch 00026: LearningRateScheduler reducing learning rate to 1.3410686196639628e-06.\nEpoch 26/50\n115/117 [============================>.] - ETA: 0s - accuracy: 0.9995 - loss: 0.0018\nEpoch 00026: val_accuracy did not improve from 0.99450\n117/117 [==============================] - 5s 43ms/step - accuracy: 0.9995 - loss: 0.0018 - val_accuracy: 0.9944 - val_loss: 0.0217 - lr: 1.3411e-06\n\nEpoch 00027: LearningRateScheduler reducing learning rate to 9.38748033764774e-07.\nEpoch 27/50\n116/117 [============================>.] - ETA: 0s - accuracy: 0.9995 - loss: 0.0019\nEpoch 00027: val_accuracy did not improve from 0.99450\n117/117 [==============================] - 5s 43ms/step - accuracy: 0.9995 - loss: 0.0019 - val_accuracy: 0.9942 - val_loss: 0.0218 - lr: 9.3875e-07\n\nEpoch 00028: LearningRateScheduler reducing learning rate to 6.571236236353417e-07.\nEpoch 28/50\n116/117 [============================>.] - ETA: 0s - accuracy: 0.9995 - loss: 0.0020\nEpoch 00028: val_accuracy did not improve from 0.99450\n117/117 [==============================] - 5s 42ms/step - accuracy: 0.9995 - loss: 0.0019 - val_accuracy: 0.9944 - val_loss: 0.0216 - lr: 6.5712e-07\n\nEpoch 00029: LearningRateScheduler reducing learning rate to 4.5998653654473913e-07.\nEpoch 29/50\n115/117 [============================>.] - ETA: 0s - accuracy: 0.9995 - loss: 0.0019\nEpoch 00029: val_accuracy improved from 0.99450 to 0.99460, saving model to best_model.h5\n117/117 [==============================] - 5s 45ms/step - accuracy: 0.9995 - loss: 0.0019 - val_accuracy: 0.9946 - val_loss: 0.0212 - lr: 4.5999e-07\n\nEpoch 00030: LearningRateScheduler reducing learning rate to 3.219905755813174e-07.\nEpoch 30/50\n114/117 [============================>.] - ETA: 0s - accuracy: 0.9995 - loss: 0.0020\nEpoch 00030: val_accuracy did not improve from 0.99460\n117/117 [==============================] - 5s 43ms/step - accuracy: 0.9995 - loss: 0.0019 - val_accuracy: 0.9945 - val_loss: 0.0214 - lr: 3.2199e-07\n\nEpoch 00031: LearningRateScheduler reducing learning rate to 2.2539340290692216e-07.\nEpoch 31/50\n116/117 [============================>.] - ETA: 0s - accuracy: 0.9994 - loss: 0.0019\nEpoch 00031: val_accuracy did not improve from 0.99460\n117/117 [==============================] - 5s 42ms/step - accuracy: 0.9994 - loss: 0.0019 - val_accuracy: 0.9944 - val_loss: 0.0215 - lr: 2.2539e-07\n\nEpoch 00032: LearningRateScheduler reducing learning rate to 1.577753820348455e-07.\nEpoch 32/50\n116/117 [============================>.] - ETA: 0s - accuracy: 0.9996 - loss: 0.0015\nEpoch 00032: val_accuracy did not improve from 0.99460\n117/117 [==============================] - 5s 46ms/step - accuracy: 0.9996 - loss: 0.0015 - val_accuracy: 0.9944 - val_loss: 0.0216 - lr: 1.5778e-07\n\nEpoch 00033: LearningRateScheduler reducing learning rate to 1.1044276742439184e-07.\nEpoch 33/50\n117/117 [==============================] - ETA: 0s - accuracy: 0.9995 - loss: 0.0018\nEpoch 00033: val_accuracy did not improve from 0.99460\n117/117 [==============================] - 6s 49ms/step - accuracy: 0.9995 - loss: 0.0018 - val_accuracy: 0.9944 - val_loss: 0.0216 - lr: 1.1044e-07\n\nEpoch 00034: LearningRateScheduler reducing learning rate to 7.730993719707429e-08.\nEpoch 34/50\n115/117 [============================>.] - ETA: 0s - accuracy: 0.9994 - loss: 0.0020\nEpoch 00034: val_accuracy did not improve from 0.99460\n117/117 [==============================] - 5s 46ms/step - accuracy: 0.9995 - loss: 0.0020 - val_accuracy: 0.9944 - val_loss: 0.0218 - lr: 7.7310e-08\n\nEpoch 00035: LearningRateScheduler reducing learning rate to 5.411695603795199e-08.\nEpoch 35/50\n115/117 [============================>.] - ETA: 0s - accuracy: 0.9995 - loss: 0.0016\nEpoch 00035: val_accuracy did not improve from 0.99460\n117/117 [==============================] - 5s 42ms/step - accuracy: 0.9995 - loss: 0.0016 - val_accuracy: 0.9943 - val_loss: 0.0216 - lr: 5.4117e-08\n\nEpoch 00036: LearningRateScheduler reducing learning rate to 3.7881869226566394e-08.\nEpoch 36/50\n116/117 [============================>.] - ETA: 0s - accuracy: 0.9995 - loss: 0.0017\nEpoch 00036: val_accuracy did not improve from 0.99460\n117/117 [==============================] - 5s 42ms/step - accuracy: 0.9995 - loss: 0.0017 - val_accuracy: 0.9945 - val_loss: 0.0215 - lr: 3.7882e-08\n\nEpoch 00037: LearningRateScheduler reducing learning rate to 2.6517308458596473e-08.\nEpoch 37/50\n117/117 [==============================] - ETA: 0s - accuracy: 0.9995 - loss: 0.0020\nEpoch 00037: val_accuracy did not improve from 0.99460\n117/117 [==============================] - 5s 43ms/step - accuracy: 0.9995 - loss: 0.0020 - val_accuracy: 0.9945 - val_loss: 0.0215 - lr: 2.6517e-08\n\nEpoch 00038: LearningRateScheduler reducing learning rate to 1.856211592101753e-08.\nEpoch 38/50\n115/117 [============================>.] - ETA: 0s - accuracy: 0.9996 - loss: 0.0015\nEpoch 00038: val_accuracy did not improve from 0.99460\n117/117 [==============================] - 5s 43ms/step - accuracy: 0.9996 - loss: 0.0016 - val_accuracy: 0.9943 - val_loss: 0.0216 - lr: 1.8562e-08\n\nEpoch 00039: LearningRateScheduler reducing learning rate to 1.299348114471227e-08.\nEpoch 39/50\n116/117 [============================>.] - ETA: 0s - accuracy: 0.9994 - loss: 0.0020\nEpoch 00039: val_accuracy did not improve from 0.99460\n117/117 [==============================] - 5s 45ms/step - accuracy: 0.9994 - loss: 0.0020 - val_accuracy: 0.9943 - val_loss: 0.0217 - lr: 1.2993e-08\n\nEpoch 00040: LearningRateScheduler reducing learning rate to 9.09543680129859e-09.\nEpoch 40/50\n116/117 [============================>.] - ETA: 0s - accuracy: 0.9995 - loss: 0.0018\nEpoch 00040: val_accuracy did not improve from 0.99460\n117/117 [==============================] - 5s 41ms/step - accuracy: 0.9995 - loss: 0.0018 - val_accuracy: 0.9942 - val_loss: 0.0216 - lr: 9.0954e-09\n\nEpoch 00041: LearningRateScheduler reducing learning rate to 6.366805760909012e-09.\nEpoch 41/50\n116/117 [============================>.] - ETA: 0s - accuracy: 0.9996 - loss: 0.0015\nEpoch 00041: val_accuracy did not improve from 0.99460\n117/117 [==============================] - 5s 42ms/step - accuracy: 0.9996 - loss: 0.0015 - val_accuracy: 0.9944 - val_loss: 0.0216 - lr: 6.3668e-09\n\nEpoch 00042: LearningRateScheduler reducing learning rate to 4.4567640326363075e-09.\nEpoch 42/50\n116/117 [============================>.] - ETA: 0s - accuracy: 0.9994 - loss: 0.0021\nEpoch 00042: val_accuracy did not improve from 0.99460\n117/117 [==============================] - 5s 42ms/step - accuracy: 0.9994 - loss: 0.0021 - val_accuracy: 0.9943 - val_loss: 0.0215 - lr: 4.4568e-09\n\nEpoch 00043: LearningRateScheduler reducing learning rate to 3.1197348228454156e-09.\nEpoch 43/50\n","name":"stdout"},{"output_type":"stream","text":"115/117 [============================>.] - ETA: 0s - accuracy: 0.9995 - loss: 0.0018\nEpoch 00043: val_accuracy did not improve from 0.99460\n117/117 [==============================] - 5s 42ms/step - accuracy: 0.9995 - loss: 0.0018 - val_accuracy: 0.9943 - val_loss: 0.0218 - lr: 3.1197e-09\n\nEpoch 00044: LearningRateScheduler reducing learning rate to 2.1838143759917907e-09.\nEpoch 44/50\n116/117 [============================>.] - ETA: 0s - accuracy: 0.9995 - loss: 0.0019\nEpoch 00044: val_accuracy did not improve from 0.99460\n117/117 [==============================] - 5s 43ms/step - accuracy: 0.9995 - loss: 0.0019 - val_accuracy: 0.9943 - val_loss: 0.0216 - lr: 2.1838e-09\n\nEpoch 00045: LearningRateScheduler reducing learning rate to 1.5286700631942536e-09.\nEpoch 45/50\n114/117 [============================>.] - ETA: 0s - accuracy: 0.9995 - loss: 0.0020\nEpoch 00045: val_accuracy did not improve from 0.99460\n117/117 [==============================] - 5s 44ms/step - accuracy: 0.9995 - loss: 0.0020 - val_accuracy: 0.9944 - val_loss: 0.0217 - lr: 1.5287e-09\n\nEpoch 00046: LearningRateScheduler reducing learning rate to 1.0700690442359773e-09.\nEpoch 46/50\n116/117 [============================>.] - ETA: 0s - accuracy: 0.9996 - loss: 0.0016\nEpoch 00046: val_accuracy did not improve from 0.99460\n117/117 [==============================] - 5s 42ms/step - accuracy: 0.9996 - loss: 0.0016 - val_accuracy: 0.9945 - val_loss: 0.0217 - lr: 1.0701e-09\n\nEpoch 00047: LearningRateScheduler reducing learning rate to 7.490483309651841e-10.\nEpoch 47/50\n114/117 [============================>.] - ETA: 0s - accuracy: 0.9996 - loss: 0.0018\nEpoch 00047: val_accuracy did not improve from 0.99460\n117/117 [==============================] - 5s 42ms/step - accuracy: 0.9995 - loss: 0.0018 - val_accuracy: 0.9944 - val_loss: 0.0217 - lr: 7.4905e-10\n\nEpoch 00048: LearningRateScheduler reducing learning rate to 5.243338316756289e-10.\nEpoch 48/50\n115/117 [============================>.] - ETA: 0s - accuracy: 0.9996 - loss: 0.0015\nEpoch 00048: val_accuracy did not improve from 0.99460\n117/117 [==============================] - 5s 42ms/step - accuracy: 0.9996 - loss: 0.0015 - val_accuracy: 0.9945 - val_loss: 0.0216 - lr: 5.2433e-10\n\nEpoch 00049: LearningRateScheduler reducing learning rate to 3.6703368217294017e-10.\nEpoch 49/50\n116/117 [============================>.] - ETA: 0s - accuracy: 0.9996 - loss: 0.0016\nEpoch 00049: val_accuracy did not improve from 0.99460\n117/117 [==============================] - 5s 42ms/step - accuracy: 0.9996 - loss: 0.0016 - val_accuracy: 0.9944 - val_loss: 0.0215 - lr: 3.6703e-10\n\nEpoch 00050: LearningRateScheduler reducing learning rate to 2.569235775210581e-10.\nEpoch 50/50\n115/117 [============================>.] - ETA: 0s - accuracy: 0.9994 - loss: 0.0021\nEpoch 00050: val_accuracy did not improve from 0.99460\n117/117 [==============================] - 5s 42ms/step - accuracy: 0.9994 - loss: 0.0021 - val_accuracy: 0.9945 - val_loss: 0.0215 - lr: 2.5692e-10\n1/1 [==============================] - 0s 2ms/step - accuracy: 0.9945 - loss: 0.0215\nValidation accuracy:  0.9944999814033508\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# 6. Evaluate the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"score = model.evaluate(x_test,y_test,verbose=0)\nscore","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"[0.015627268701791763, 0.995199978351593]"},"metadata":{}}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}